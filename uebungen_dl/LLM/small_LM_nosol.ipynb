{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Language Models\n",
    "\n",
    "This notebook explores several neural network architectures for language modeling tasks. The models are trained on an artificially generated language. \n",
    "\n",
    "## Background and Motivation\n",
    "Training language models on real-world language data often requires extensive computational resources typically found in data centers. To explore the fundamental concepts of language modeling in a more accessible manner, we will use an artificially constructed language. This approach allows us to control complexity, understand the underlying mechanics, and ensure resource efficiency.\n",
    "\n",
    "## Objective\n",
    "Understand, implement, and compare different neural network architectures (RNN, LSTM, Transformer) in language modeling tasks using an artificially generated language.\n",
    "\n",
    "## Tasks\n",
    "1. **Artificial Language Generation for Training**: Generate an initial dataset of sentences using a simple, rule-based grammar (see below).\n",
    "2. **Tokenization**: Convert these sentences into sequences of integers.\n",
    "3. **Model Building**: Implement 4 models models - a basic RNN, an LSTM, 1DCNN and a Transformer.\n",
    "4. **Training**: Train each model on the artificial language dataset and evaluate the performance of each model while training on the validation set.\n",
    "\n",
    "You can use the starter code provided below to get started. Feel free to modify the code as you see fit. The architecture of the models is up to you but can be simple. For example for the RNN it might look like this:\n",
    "\n",
    "| Layer (type)            | Output Shape      | Param # |\n",
    "|-------------------------|-------------------|---------|\n",
    "| embedding_1 (Embedding) | (1, 35, 8)        | 200     |\n",
    "| simple_rnn (SimpleRNN)  | (1, 50)           | 2,950   |\n",
    "| dense (Dense)           | (1, 25)           | 1,275   |\n",
    "\n",
    "Important is to choose an appropriate loss function for the next predicted token. In the above example, the vocabulary size is 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Define the grammar rules\n",
    "expanded_grammar = {\n",
    "    'S': [['NP', 'VP'], ['S', 'CONJ', 'S']],\n",
    "    'NP': [['Det', 'ADJ', 'N'], ['Det', 'N'], ['Det', 'N', 'PP']],\n",
    "    'VP': [['V', 'NP'], ['V', 'NP', 'PP'], ['V', 'ADV']],\n",
    "    'PP': [['P', 'NP']],\n",
    "    'Det': ['a', 'the'],\n",
    "    'N': ['cat', 'dog', 'bird', 'tree'],\n",
    "    'V': ['sits', 'runs', 'flies', 'jumps'],\n",
    "    'ADJ': ['big', 'small', 'quick', 'lazy'],\n",
    "    'ADV': ['quickly', 'slowly', 'carefully'],\n",
    "    'P': ['on', 'in', 'under', 'over'],\n",
    "    'CONJ': ['and', 'but', 'or']\n",
    "}\n",
    "\n",
    "def generate_sentence(symbol, depth=0, max_depth=5):\n",
    "    if depth > max_depth:  # Limiting recursion depth\n",
    "        return ''\n",
    "\n",
    "    if symbol not in expanded_grammar:\n",
    "        return symbol\n",
    "\n",
    "    expansion = random.choice(expanded_grammar[symbol])\n",
    "    if isinstance(expansion, list):\n",
    "        return ' '.join([generate_sentence(sym, depth+1, max_depth) for sym in expansion]).strip()\n",
    "    else:\n",
    "        return expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data (Tokenization and Padding)\n",
    "\n",
    "In this first step, we will tokenize the data and pad it to the maximum length of the sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first sentence :'the small dog sits a tree on a small cat'\n",
      "The first sentence after tokanization and padding :\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1 17  6 12  2  3 14  2 17  5]\n",
      "The first sentence X for training (note the last token is missing):\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1 17  6 12  2  3 14  2 17]\n",
      "The first sentence Y for training (note that this is one-hot-encoded):\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "Shape of X (Batch, T=35, V=25)\n",
      "Shape of X (in code)): (1000, 35), shape of y: (1000, 25)\n"
     ]
    }
   ],
   "source": [
    "random.seed(42) # For reproducibility\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Generating a sentences of sentences\n",
    "num_train = 1000                        # Number of training examples\n",
    "num_test  = 2000                        # Number of test examples\n",
    "num_sentences = num_train + num_test    # Number of sentences to generate\n",
    "sentences = [generate_sentence('S') for _ in range(num_sentences)]\n",
    "\n",
    "# Creating tokens from sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Preparing data for model input \n",
    "vocab_size = len(tokenizer.word_index) + 1  # Plus 1 for padding\n",
    "\n",
    "# Pad all sequences to the same length\n",
    "max_length = max(len(s) for s in sequences)\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "\n",
    "# Prepare X, y for training\n",
    "X = sequences[0:num_train,:-1]\n",
    "y = to_categorical(sequences[0:num_train,-1], num_classes=vocab_size)\n",
    "\n",
    "# Prepare X, y for testing\n",
    "X_test = sequences[num_train:,:-1]\n",
    "y_test = to_categorical(sequences[num_train:,-1], num_classes=vocab_size)\n",
    "\n",
    "# The first sentence\n",
    "print(f\"The first sentence :\\'{sentences[0]}\\'\")\n",
    "print(f\"The first sentence after tokanization and padding :\\n{sequences[0]}\")\n",
    "print(f\"The first sentence X for training (note the last token is missing):\\n{X[0]}\") \n",
    "print(f\"The first sentence Y for training (note that this is one-hot-encoded):\\n{y[0]}\") \n",
    "\n",
    "T = X.shape[1]  # Length of input sequence\n",
    "V = y.shape[1]  # Vocabolary size\n",
    "print(f\"Shape of X (Batch, T={T}, V={V})\")\n",
    "print(f\"Shape of X (in code)): {X.shape}, shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following shapes\n",
    "\n",
    "**X**\n",
    "Note that `X` is a sparse representation in the code. Instead of one-hot-coding the tokens, we are using the token index. This is a more efficient representation of the data, but conceptually it is the same.\n",
    "\n",
    "**y**\n",
    "y has the same Batch, Vocabulary. It is the one-hot encoded result of the next token. Note that, here are are using the **last predicted word**. This makes it easier in our code but is less efficient. In technical realizations people also use the word for all $t=1..T$ as the targets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding layer\n",
    "The first step, we do from $X(B,T,V)$ to $X(B,T,C)$ is the embedding layer. This is a simple matrix multiplication. The size of the embedding layer `C` is a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 459us/step\n",
      "Shape (1000, 35, 8)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "EMB_DIM = 8  # Embedding dimension (a.k.a. C)\n",
    "emb_model = Sequential()\n",
    "emb_model.add(Embedding(vocab_size, EMB_DIM)) \n",
    "print(f\"Shape {emb_model.predict(X).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------- End of Starter Code ------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl_htwg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
