{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Language Models\n",
    "\n",
    "This notebook explores several neural network architectures for language modeling tasks. The models are trained on an artificially generated language. \n",
    "\n",
    "## Background and Motivation\n",
    "Training language models on real-world language data often requires extensive computational resources typically found in data centers. To explore the fundamental concepts of language modeling in a more accessible manner, we will use an artificially constructed language. This approach allows us to control complexity, understand the underlying mechanics, and ensure resource efficiency.\n",
    "\n",
    "## Objective\n",
    "Understand, implement, and compare different neural network architectures (RNN, LSTM, Transformer) in language modeling tasks using an artificially generated language.\n",
    "\n",
    "## Tasks\n",
    "1. **Artificial Language Generation for Training**: Generate an initial dataset of sentences using a simple, rule-based grammar (see below).\n",
    "2. **Tokenization**: Convert these sentences into sequences of integers.\n",
    "3. **Model Building**: Implement at least the first 3 models - a basic RNN (required), an LSTM (required), 1DCNN (required) and a Transformer (optional)\n",
    "4. **Training**: Train each model on the artificial language dataset and evaluate the performance of each model while training on the validation set by showing the learning curves for each model and comparing them.\n",
    "\n",
    "You can use the starter code provided below to get started. Feel free to modify the code as you see fit. The architecture of the models is up to you but can be simple. For example for the RNN it might look like this:\n",
    "\n",
    "| Layer (type)            | Output Shape      | Param # |\n",
    "|-------------------------|-------------------|---------|\n",
    "| embedding_1 (Embedding) | (1, 35, 8)        | 200     |\n",
    "| simple_rnn (SimpleRNN)  | (1, 50)           | 2,950   |\n",
    "| dense (Dense)           | (1, 25)           | 1,275   |\n",
    "\n",
    "Important is to choose an appropriate loss function for the next predicted token. In the above example, the vocabulary size is 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Define the grammar rules\n",
    "expanded_grammar = {\n",
    "    'S': [['NP', 'VP'], ['S', 'CONJ', 'S']],\n",
    "    'NP': [['Det', 'ADJ', 'N'], ['Det', 'N'], ['Det', 'N', 'PP']],\n",
    "    'VP': [['V', 'NP'], ['V', 'NP', 'PP'], ['V', 'ADV']],\n",
    "    'PP': [['P', 'NP']],\n",
    "    'Det': ['a', 'the'],\n",
    "    'N': ['cat', 'dog', 'bird', 'tree'],\n",
    "    'V': ['sits', 'runs', 'flies', 'jumps'],\n",
    "    'ADJ': ['big', 'small', 'quick', 'lazy'],\n",
    "    'ADV': ['quickly', 'slowly', 'carefully'],\n",
    "    'P': ['on', 'in', 'under', 'over'],\n",
    "    'CONJ': ['and', 'but', 'or']\n",
    "}\n",
    "\n",
    "def generate_sentence(symbol, depth=0, max_depth=5):\n",
    "    if depth > max_depth:  # Limiting recursion depth\n",
    "        return ''\n",
    "\n",
    "    if symbol not in expanded_grammar:\n",
    "        return symbol\n",
    "\n",
    "    expansion = random.choice(expanded_grammar[symbol])\n",
    "    if isinstance(expansion, list):\n",
    "        return ' '.join([generate_sentence(sym, depth+1, max_depth) for sym in expansion]).strip()\n",
    "    else:\n",
    "        return expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data (Tokenization and Padding)\n",
    "\n",
    "In this first step, we will tokenize the data and pad it to the maximum length of the sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first sentence :'the small dog sits a tree on a small cat'\n",
      "The first sentence after tokanization and padding :\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1 17  6 12  2  3 14  2 17  5]\n",
      "The first sentence X for training (note the last token is missing):\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1 17  6 12  2  3 14  2 17]\n",
      "The first sentence Y for training (note that this is one-hot-encoded):\n",
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0.]\n",
      "Shape of X (Batch, T=35, V=25)\n",
      "Shape of X (in code)): (1000, 35), shape of y: (1000, 25)\n"
     ]
    }
   ],
   "source": [
    "random.seed(42) # For reproducibility\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Generating a sentences of sentences\n",
    "num_train = 1000                        # Number of training examples\n",
    "num_test  = 2000                        # Number of test examples\n",
    "num_sentences = num_train + num_test    # Number of sentences to generate\n",
    "sentences = [generate_sentence('S') for _ in range(num_sentences)]\n",
    "\n",
    "# Creating tokens from sentences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "# Preparing data for model input \n",
    "vocab_size = len(tokenizer.word_index) + 1  # Plus 1 for padding\n",
    "\n",
    "# Pad all sequences to the same length\n",
    "max_length = max(len(s) for s in sequences)\n",
    "sequences = pad_sequences(sequences, maxlen=max_length, padding='pre')\n",
    "\n",
    "# Prepare X, y for training\n",
    "X = sequences[0:num_train,:-1]\n",
    "y = to_categorical(sequences[0:num_train,-1], num_classes=vocab_size)\n",
    "\n",
    "# Prepare X, y for testing\n",
    "X_test = sequences[num_train:,:-1]\n",
    "y_test = to_categorical(sequences[num_train:,-1], num_classes=vocab_size)\n",
    "\n",
    "# The first sentence\n",
    "print(f\"The first sentence :\\'{sentences[0]}\\'\")\n",
    "print(f\"The first sentence after tokanization and padding :\\n{sequences[0]}\")\n",
    "print(f\"The first sentence X for training (note the last token is missing):\\n{X[0]}\") \n",
    "print(f\"The first sentence Y for training (note that this is one-hot-encoded):\\n{y[0]}\") \n",
    "\n",
    "T = X.shape[1]  # Length of input sequence\n",
    "V = y.shape[1]  # Vocabolary size\n",
    "print(f\"Shape of X (Batch, T={T}, V={V})\")\n",
    "print(f\"Shape of X (in code)): {X.shape}, shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following shapes\n",
    "\n",
    "**X**\n",
    "Note that `X` is a sparse representation in the code. Instead of one-hot-coding the tokens, we are using the token index. This is a more efficient representation of the data, but conceptually it is the same.\n",
    "\n",
    "**y**\n",
    "y has the same Batch, Vocabulary. It is the one-hot encoded result of the next token. Note that, here are are using the **last predicted word**. This makes it easier in our code but is less efficient. In technical realizations people also use the word for all $t=1..T$ as the targets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding layer\n",
    "The first step, we do from $X(B,T,V)$ to $X(B,T,C)$ is the embedding layer. This is a simple matrix multiplication. The size of the embedding layer `C` is a hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 459us/step\n",
      "Shape (1000, 35, 8)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "EMB_DIM = 8  # Embedding dimension (a.k.a. C)\n",
    "emb_model = Sequential()\n",
    "emb_model.add(Embedding(vocab_size, EMB_DIM)) \n",
    "print(f\"Shape {emb_model.predict(X).shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Hints\n",
    "#### Basic RNN\n",
    "\n",
    "In the basic RNN model, you can use the `SimpleRNN` layer. The beginning of the code is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Model\n",
    "rnn_model = Sequential()\n",
    "rnn_model.add(Embedding(vocab_size, EMB_DIM))           # Embedding layer\n",
    "##### YOUR CODE HERE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM\n",
    "Adpot the code from the basic RNN and replace the `SimpleRNN` layer with an `LSTM` layer. \n",
    "\n",
    "#### 1DCNN\n",
    "Start same as LSTM and RNN. Replace the `LSTM` layer with serval `Conv1D` layer as shown in the lecture slides. The last layer should only return a single value. This can be achived with \n",
    "```python\n",
    "cnn_model.add(Lambda(lambda x: x[:, -1, :]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention\n",
    "Here we use a single Attention block. The code is given below. \n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, MultiHeadAttention, Dense, LayerNormalization, Dropout, Embedding\n",
    "\n",
    "\n",
    "MAX_LEN = T\n",
    "FF_DIM = 2* EMB_DIM  # Hidden layer size in the feedforward network (after attention)\n",
    "NUM_HEADS = 2 # Number of attention heads\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(MAX_LEN,))\n",
    "embedding_layer = Embedding(vocab_size, EMB_DIM)(input_layer)\n",
    "pos_encoding_layer = embedding_layer#PositionalEncoding(EMB_DIM)(embedding_layer)\n",
    "\n",
    "attention_output = MultiHeadAttention(num_heads=NUM_HEADS, key_dim=EMB_DIM)(pos_encoding_layer, pos_encoding_layer)\n",
    "attention_output = Dropout(0.1)(attention_output)\n",
    "attention_output = LayerNormalization(epsilon=1e-6)(attention_output + pos_encoding_layer)\n",
    "\n",
    "feed_forward_output = Dense(FF_DIM, activation='relu')(attention_output)\n",
    "feed_forward_output = Dense(EMB_DIM)(feed_forward_output)\n",
    "feed_forward_output = Dropout(0.1)(feed_forward_output)\n",
    "feed_forward_output = LayerNormalization(epsilon=1e-6)(feed_forward_output + attention_output)\n",
    "\n",
    "# Output layer (is still a tensor of shape (batch_size, T, EMB_DIM)\n",
    "# We just need the last word, so we select it with [:,-1,:]\n",
    "output_layer = Dense(vocab_size, activation='softmax')(feed_forward_output[:,-1,:])\n",
    "\n",
    "\n",
    "\n",
    "# Create the model\n",
    "transformer_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "# Compile the model\n",
    "transformer_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "transformer_model.predict(X[1].reshape(1,-1))  # Predict the first sentence (needs to be called)\n",
    "```\n",
    "\n",
    "How does this code differ from the diagram in the lecture slides below?\n",
    "\n",
    "![Attention](trafo.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------- End of Starter Code ------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl_htwg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
