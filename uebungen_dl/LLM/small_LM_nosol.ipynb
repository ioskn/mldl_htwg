{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Small Language Models\n",
    "This notebook explores several neural network architectures for language modeling tasks. The models are trained on an artificially generated language. To the task remains the same. To predict the probability of the next word $Y$ given the previous words $X$ in a sentence.\n",
    "\n",
    "$$\n",
    "P(Y | X) \n",
    "$$\n",
    "\n",
    "## Background and Motivation\n",
    "Training language models on real-world language data often requires extensive computational resources typically found in data centers. To explore the fundamental concepts of language modeling in a more accessible manner, we will use an artificially constructed language. This approach allows us to control complexity, understand the underlying mechanics, and ensure resource efficiency.\n",
    "\n",
    "## Objective\n",
    "Understand, implement, and compare different neural network architectures (RNN, LSTM, Transformer) in language modeling tasks using an artificially generated language.\n",
    "\n",
    "## Tasks\n",
    "1. **Artificial Language Generation for Training**: Generate an initial dataset of sentences using a simple, rule-based grammar (see below).\n",
    "2. **Tokenization**: Convert these sentences into sequences of integers.\n",
    "3. **Model Building**: Implement at least the first 3 models - a basic RNN (required), an LSTM (required), 1DCNN (required) and a Transformer (optional)\n",
    "4. **Training**: Train each model on the artificial language dataset and evaluate the performance of each model while training on the validation set by showing the learning curves for each model and comparing them.\n",
    "\n",
    "You can use the starter code provided below to get started. Feel free to modify the code as you see fit. The architecture of the models is up to you but can be simple. For example for the RNN it might look like this:\n",
    "\n",
    "| Layer (type)            | Output Shape      | Param # |\n",
    "|-------------------------|-------------------|---------|\n",
    "| embedding_1 (Embedding) | (1, 35, 8)        | 208     |\n",
    "| simple_rnn (SimpleRNN)  | (1, 50)           | 2,950   |\n",
    "| dense (Dense)           | (1, 26)           | 1,275   |\n",
    "\n",
    "Important is to choose an appropriate loss function for the next predicted token. In the above example, the vocabulary size is 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.26\n",
      "3.6.0\n"
     ]
    }
   ],
   "source": [
    "################# Keras with JAX Backend ################\n",
    "##### We use the JAX backend for Keras\n",
    "import jax \n",
    "print(jax.__version__)\n",
    "\n",
    "##### We set the KERAS_BACKEND environment variable to \"jax\"\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "##### We import Keras\n",
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strange Language\n",
    "\n",
    "We use the following procedure to generate samples from the language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# Define the grammar rules\n",
    "expanded_grammar = {\n",
    "    'S': [['NP', 'VP'], ['S', 'CONJ', 'S']],\n",
    "    'NP': [['Det', 'ADJ', 'N'], ['Det', 'N'], ['Det', 'N', 'PP']],\n",
    "    'VP': [['V', 'NP'], ['V', 'NP', 'PP'], ['V', 'ADV']],\n",
    "    'PP': [['P', 'NP']],\n",
    "    'Det': ['a', 'the'],\n",
    "    'N': ['cat', 'dog', 'bird', 'tree'],\n",
    "    'V': ['sits', 'runs', 'flies', 'jumps'],\n",
    "    'ADJ': ['big', 'small', 'quick', 'lazy'],\n",
    "    'ADV': ['quickly', 'slowly', 'carefully'],\n",
    "    'P': ['on', 'in', 'under', 'over'],\n",
    "    'CONJ': ['and', 'but', 'or']\n",
    "}\n",
    "\n",
    "def generate_sentence(symbol, depth=0, max_depth=5):\n",
    "    if depth > max_depth:  # Limiting recursion depth\n",
    "        return ''\n",
    "\n",
    "    if symbol not in expanded_grammar:\n",
    "        return symbol\n",
    "\n",
    "    expansion = random.choice(expanded_grammar[symbol])\n",
    "    if isinstance(expansion, list):\n",
    "        return ' '.join([generate_sentence(sym, depth+1, max_depth) for sym in expansion]).strip()\n",
    "    else:\n",
    "        return expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation of Training and Test Data \n",
    "\n",
    "Here we generate the training and test data for the language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 the small dog sits a tree on a small cat\n",
      "1 the dog under a lazy bird runs the big cat on the bird\n",
      "2 the cat on the bird on runs slowly\n",
      "3 a lazy bird flies the quick dog on a dog over\n",
      "4 the cat on the bird runs the dog over but a dog flies carefully\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generating sentences\n",
    "num_train = 1000                        # Number of training examples\n",
    "num_test  = 2000                        # Number of test examples\n",
    "num_sentences = num_train + num_test    # Number of sentences to generate\n",
    "sentences = [generate_sentence('S') for _ in range(num_sentences)]\n",
    "\n",
    "# Sample some sentences\n",
    "for i in range(5):\n",
    "    print(f\"{i} {sentences[i]}\")\n",
    "\n",
    "max_length = max(len(sentence.split()) for sentence in sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "In the Tokenization step, we convert the input text into integer numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2 18  7 13  3  4 15  3 18  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2  7 16  3 22  5 12  2 20  6 15  2  5  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 2  6 15  2  5 15 12 24  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 3 22  5 14  2 19  7 15  3  7 17  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "# Define TextVectorization layer\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=40, # Maximum vocabulary size (will be less)\n",
    "    output_mode=\"int\"\n",
    ")\n",
    "\n",
    "# Adapt the vectorization layer\n",
    "vectorize_layer.adapt(sentences)\n",
    "\n",
    "# Vectorize sentences (output is JAX-compatible)\n",
    "sequences = vectorize_layer(sentences)\n",
    "print(sequences[:4])\n",
    "\n",
    "vocab_size = len(vectorize_layer.get_vocabulary())\n",
    "print(vocab_size)\n",
    "\n",
    "\n",
    "# get frequency of each word\n",
    "#from collections import Counter\n",
    "#word_counts = Counter()\n",
    "#for sentence in sentences:\n",
    "#    word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "Since the input text can have variable length, the tokenization added 0 at the end of the text to make all the text of the same length. However, we want that the text ends with a real token. So we take the 0 at the end and move it to the beginning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  2 18  7 13  3  4 15  3 18  6]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2\n",
      "   7 16  3 22  5 12  2 20  6 15  2  5]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  2  6 15  2  5 15 12 24]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  3 22  5 14  2 19  7 15  3  7 17]]\n"
     ]
    }
   ],
   "source": [
    "# Function to shift padding tokens to the beginning\n",
    "def move_padding_to_front(sequence, padding_value=0):\n",
    "    non_padding = sequence[sequence != padding_value]\n",
    "    padding = sequence[sequence == padding_value]\n",
    "    return jnp.concatenate([padding, non_padding])\n",
    "\n",
    "# Apply the function to all sequences\n",
    "adjusted_sequences = jnp.array([move_padding_to_front(seq) for seq in sequences])\n",
    "\n",
    "# Print adjusted sequences\n",
    "print(adjusted_sequences[:4])\n",
    "del(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Test Data\n",
    "The first part of the sentences is used for training data the aim is to predict the last token of the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first sentence :'the small dog sits a tree on a small cat'\n",
      "The first sentence after tokenization and padding :\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  2 18  7 13  3  4 15  3 18  6]\n",
      "The first sentence X for training (note the last token is missing):\n",
      "[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  2 18  7 13  3  4 15  3 18]\n",
      "The first sentence Y for training (note that this is one-hot-encoded):\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "Shape of X: (1000, 35)\n",
      "Shape of Y: (1000, 26)\n"
     ]
    }
   ],
   "source": [
    "# Prepare X and y for training\n",
    "X = adjusted_sequences[:num_train, :-1]\n",
    "Y = keras.utils.to_categorical(adjusted_sequences[:num_train, -1], num_classes=vocab_size)\n",
    "\n",
    "# Prepare X_test and y_test\n",
    "X_test = adjusted_sequences[num_train:, :-1]\n",
    "y_test = keras.utils.to_categorical(adjusted_sequences[num_train:, -1], num_classes=vocab_size)\n",
    "\n",
    "# Changes end here\n",
    "# The first sentence\n",
    "print(f\"The first sentence :\\'{sentences[0]}\\'\")\n",
    "print(f\"The first sentence after tokenization and padding :\\n{adjusted_sequences[0]}\")\n",
    "print(f\"The first sentence X for training (note the last token is missing):\\n{X[0]}\")\n",
    "print(f\"The first sentence Y for training (note that this is one-hot-encoded):\\n{Y[0]}\") \n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of Y: {Y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapes and Embeddings\n",
    "`X` is a sparse representation in the code. Instead of one-hot-coding the tokens, we are using the token index, i.e. every word / token corresponds to an interger valu. This is a more efficient representation of the data, but conceptually it is the same.\n",
    "\n",
    "`Y` is one hot encoded has the shape Batch, Vocabulary. It is the one-hot encoded result of the next token. Note that, here we are using the **last predicted word**. This makes it easier in our code but is less efficient. In technical realizations people also use the word for all $t=1..T$ as the targets.\n",
    "\n",
    "#### Embedding layer\n",
    "The first step, we do from $X(B,T,V)$ to $X(B,T,C)$ is the embedding layer. This is a simple matrix multiplication. The size of the embedding layer `C` is a hyperparameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Shape (1000, 35, 8, 8, 8) (Batch, Time, Embedding)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "EMB_DIM = 8  # Embedding dimension (a.k.a. C)\n",
    "# Define the model\n",
    "\n",
    "emb_model.add(Embedding(vocab_size, EMB_DIM)) \n",
    "### \n",
    "# <---- Add your model here ---->\n",
    "###\n",
    "\n",
    "# Predict embeddings for the input data\n",
    "embeddings = emb_model.predict(X)  # X is already prepared as input sequences\n",
    "print(f\"Shape {embeddings.shape} (Batch, Time, Embedding)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the embedding layer above. We can now apply the RNN, LSTM, 1DCNN, or Transformer.\n",
    "### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM\n",
    "Adpot the code from the basic RNN and replace the `SimpleRNN` layer with an `LSTM` layer. \n",
    "\n",
    "#### 1DCNN\n",
    "Start same as LSTM and RNN. Replace the `LSTM` layer with serval `Conv1D` layer as shown in the lecture slides. The last layer should only return a single value. There are two ways to do this.\n",
    "\n",
    "If you use the Sequential API \n",
    "```python\n",
    "cnn_model.add(Lambda(lambda x: x[:, -1, :]))\n",
    "```\n",
    "\n",
    "If you use the Functional API\n",
    "```python\n",
    "output_layer = Dense(vocab_size, activation='softmax')(x[:, -1, :]) # x is the output of the last Conv1D layer\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer\n",
    "Here we use a single Attention block consisting of the following layers. As for the CNN, the last layer should only return a single value. \n",
    "\n",
    "```python\n",
    "\n",
    "###### Attention Block\n",
    "MultiHeadAttention #(B,T,C)\n",
    "LayerNormalization #(B,T,C)\n",
    "Dense              #(B,T,C)    \n",
    "##### \n",
    "# At the end we add a Dense layer to get the output\n",
    "Dense              #(B,V)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------- End of Starter Code ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl_htwg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
