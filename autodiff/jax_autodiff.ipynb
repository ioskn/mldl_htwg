{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autodiff\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/google/jax/main/images/jax_logo.svg\" alt=\"JAX Logo\" height =\"50\"/>\n",
    "\n",
    "JAX is like numpy, but with autodiff. This means that you can compute gradients of functions with respect to their inputs.\n",
    "\n",
    "\n",
    "<small>Other frameworks like TensorFlow, Pytorch also have autodiff</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Example\n",
    "\n",
    "### Standart numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x,y,z):\n",
    "    return (x+y)*z\n",
    "\n",
    "f(-2,5,-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same with JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g(-2,5,-4) = -12\n",
      "grad(g)|(-2.,5.,-4.) = -4.0\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "def g(x,y,z):\n",
    "    return (x+y)*z\n",
    "\n",
    "print(f'g(-2,5,-4) = {g(-2,5,-4)}')\n",
    "print(f'grad(g)|(-2.,5.,-4.) = {grad(g)(-2.,5.,-4.)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n",
      "32.0\n",
      "48.0\n"
     ]
    }
   ],
   "source": [
    "### Second order derivatives\n",
    "def gg(x):\n",
    "    return x**4\n",
    "\n",
    "print(f'{gg(2.)}')\n",
    "ggg = grad(gg)\n",
    "print(f'{ggg(2.)}')\n",
    "print(f'{grad(ggg)(2.)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autodiff of functions (in a coding sense)\n",
    "\n",
    "Autodiff is not limited to simple functions. You can compute gradients of complicated pythin functions with respect to their inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Sums with loops\n",
    "\n",
    "Let's look at the following function:\n",
    "$$\n",
    "f(x) = \\sum_{k=1}^{N} \\left( \\ln(x + k) - \\ln(x + k - 1) \\right)\n",
    "$$\n",
    "\n",
    "The derivative of this function w.r.t. $x$ is (feel free to try to derive it yourself):\n",
    "$$\n",
    "f^{\\prime}(x) = \\frac{1}{x + N} - \\frac{1}{x}\n",
    "$$\n",
    "\n",
    "Let's implement this function in a naive way and calculate the derivative using JAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(-0.8333334, dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def f(x, N):\n",
    "    result = 0.0\n",
    "    for k in range(1, N + 1):\n",
    "        term = jnp.log(x + k) - jnp.log(x + k - 1)\n",
    "        result += term\n",
    "    return result\n",
    "\n",
    "# Derivative respect to x, always uses the first argument\n",
    "df_dx = jax.grad(f)\n",
    "\n",
    "# Example values\n",
    "x_value = 1.0\n",
    "N_value = 5\n",
    "\n",
    "df_dx(x_value, N_value) # Should be 1/(1 + N) - 1 = 1/6 - 1 ~ -0.8333333 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom functions / Idea of gradient calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next examples shows how to JAX calulates the gradient. Every function needs to provide a gradient. For simple functions this is already implemented. Let's define a custom function in which we define the gradient ourselves. We take a look at the ReLU-like function, which is quadratic for positive values and 0 for negative values. We define the function as follows:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{qlu}(x) = \n",
    "\\begin{cases} \n",
    "x^2, & \\text{if } x > 0 \\\\\n",
    "0, & \\text{if } x \\leq 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "Which is continuous everywhere except at 0 but the gradient is not defined at 0. We can define the gradient to be 0.42 at 0. The flowing diagram shows the function and its gradient. \n",
    "\n",
    "<center>\n",
    "    <img src=\"qlu.svg\" alt=\"QLU Diagram\" height=\"100\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "To be used in the \"Chain Rule\" we need to define the gradient of the function w.r.t. x. This is a function that takes the value of x and the \"incomming gradient\" grad_y and returns the gradient of the function w.r.t. x. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom ReLU values: [1. 0. 4.]\n",
      "Gradients of Custom ReLU at test points: [0.   0.42 4.  ]\n"
     ]
    }
   ],
   "source": [
    "from jax import custom_vjp\n",
    "# Define the custom qlu function and state that, we provide custom gradients. \n",
    "# By defining the forward and backward functions.\n",
    "@custom_vjp\n",
    "def qlu(x):\n",
    "    return jnp.maximum(0, x**2)\n",
    "\n",
    "# Forward function: computes the output and any intermediate values\n",
    "def qlu_fwd(x):\n",
    "    y = qlu(x) \n",
    "    return y, x  # Pass x to the backward function for gradient handling\n",
    "\n",
    "# Backward function: provides a custom gradient rule\n",
    "def qlu_bwd(x, grad_y):\n",
    "    # Gradient is:\n",
    "    # 0 for x < 0\n",
    "    # 1 for x > 0\n",
    "    # 0.42 at x = 0\n",
    "    grad_x = jnp.where(x > 0, 2*x*grad_y, 0)  # Grad = x for x > 0\n",
    "    grad_x = jnp.where(x == 0, grad_y * 0.42, grad_x)  # Grad = 0.42 for x == 0\n",
    "    return (grad_x,)\n",
    "\n",
    "# Register forward and backward pass\n",
    "qlu.defvjp(qlu_fwd, qlu_bwd)\n",
    "\n",
    "# Test the function and its gradient\n",
    "x_values = jnp.array([-1.0, 0.0, 2.0])  # Test points\n",
    "print(\"Custom ReLU values:\", qlu(x_values))\n",
    "\n",
    "# Compute gradients\n",
    "grads = jax.grad(lambda x: qlu(x).sum())(x_values)\n",
    "print(\"Gradients of Custom ReLU at test points:\", grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl_htwg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
