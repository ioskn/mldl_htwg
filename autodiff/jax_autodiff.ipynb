{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autodiff\n",
    "\n",
    "JAX is like numpy, but with autodiff. This means that you can compute gradients of functions with respect to their inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standart numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x,y,z):\n",
    "    return (x+y)*z\n",
    "\n",
    "f(-2,5,-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same with JAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(-4., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Same in jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "def g(x,y,z):\n",
    "    return (x+y)*z\n",
    "\n",
    "g(-2,5,-4)\n",
    "# Gradient of g with respect to x\n",
    "grad(g)(-2.,5.,-4.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141733920000.0\n",
      "141733920000.0\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Benutzerdefinierte Funktion\n",
    "def custom_func(x):\n",
    "    return x**3\n",
    "\n",
    "# Vorwärtsfunktion: Gibt den Ausgangswert und (optionale) Zwischenwerte zurück\n",
    "def custom_func_fwd(x):\n",
    "    y = x**33\n",
    "    return y, x  # x wird an die Rückwärtsfunktion weitergegeben\n",
    "\n",
    "# Rückwärtsfunktion: Berechnet den Gradienten\n",
    "def custom_func_bwd(x, grad_y):\n",
    "    grad_x = 33 * x**32 * grad_y\n",
    "    return (grad_x,)\n",
    "\n",
    "# Registrierung der Ableitung\n",
    "custom_func = jax.custom_vjp(custom_func)\n",
    "custom_func.defvjp(custom_func_fwd, custom_func_bwd)\n",
    "\n",
    "# Test der Gradientenberechnung\n",
    "print(jax.grad(custom_func)(2.0))  # Ausgabe: 12.0\n",
    "\n",
    "#### Same w/o custom_vjp\n",
    "def func2(x):\n",
    "    return x**33\n",
    "\n",
    "print(jax.grad(func2)(2.0))  # Ausgabe: 12.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom gradients\n",
    "\n",
    "The next examples show how to compute gradients with JAX. Relu at 0 is not differentiable and the gradient is not defined. In the standard implementation, the gradient is set to 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X values: [-1.  0.  1.]\n",
      "ReLU values: [0. 0. 1.]\n",
      "Gradients of ReLU at test points: [0.  0.5 1. ]\n"
     ]
    }
   ],
   "source": [
    "# Define the ReLU function\n",
    "def relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "# Compute the gradient of ReLU\n",
    "x_values = jnp.array([-1.0, 0.0, 1.0])  # Test inputs\n",
    "grads = jax.grad(lambda x: jnp.sum(relu(x)))(x_values)\n",
    "\n",
    "# Print results\n",
    "print(\"X values:\", (x_values))\n",
    "print(\"ReLU values:\", relu(x_values))\n",
    "print(\"Gradients of ReLU at test points:\", grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's define another value for the gradient at 0\n",
    "\n",
    "The following code shows how to define a custom gradient for the relu function with gradient at 0 abitrarely defined as 0.42.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom ReLU values: [0. 0. 1.]\n",
      "Gradients of Custom ReLU at test points: [0.   0.42 1.  ]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "# Define the custom ReLU function\n",
    "def custom_relu(x):\n",
    "    return jnp.maximum(0, x)\n",
    "\n",
    "# Forward function: computes the output and any intermediate values\n",
    "def custom_relu_fwd(x):\n",
    "    return custom_relu(x), x  # Pass x to the backward function for gradient handling\n",
    "\n",
    "# Backward function: provides a custom gradient rule\n",
    "def custom_relu_bwd(x, grad_y):\n",
    "    # Gradient is:\n",
    "    # - 0 for x < 0\n",
    "    # - 1 for x > 0\n",
    "    # - 0.42 at x = 0\n",
    "    grad_x = jnp.where(x > 0, grad_y, 0)  # Grad = 1 for x > 0\n",
    "    grad_x = jnp.where(x == 0, grad_y * 0.42, grad_x)  # Grad = 0.42 for x == 0\n",
    "    return (grad_x,)\n",
    "\n",
    "# Register the custom VJP\n",
    "custom_relu = jax.custom_vjp(custom_relu)\n",
    "custom_relu.defvjp(custom_relu_fwd, custom_relu_bwd)\n",
    "\n",
    "# Test the function and its gradient\n",
    "x_values = jnp.array([-1.0, 0.0, 1.0])  # Test points\n",
    "print(\"Custom ReLU values:\", custom_relu(x_values))\n",
    "\n",
    "# Compute gradients\n",
    "grads = jax.grad(lambda x: jnp.sum(custom_relu(x)))(x_values)\n",
    "print(\"Gradients of Custom ReLU at test points:\", grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complicated functions\n",
    "\n",
    "You can also autodiff any function, containing loops, conditionals, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TracerBoolConversionError",
     "evalue": "Attempted boolean conversion of traced array with shape bool[]..\nThis BatchTracer with object id 5872364544 was created on line:\n  /var/folders/bk/0vv7sh9n43n3dm4fth1qw93r0000gq/T/ipykernel_84478/168064051.py:7 (complicated_func)\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTracerBoolConversionError\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Compute the gradient of the function\u001b[39;00m\n\u001b[1;32m     22\u001b[0m x_values \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m2.0\u001b[39m, \u001b[38;5;241m4.0\u001b[39m, \u001b[38;5;241m5.0\u001b[39m])  \u001b[38;5;66;03m# Test inputs\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv_complicated_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction values:\u001b[39m\u001b[38;5;124m\"\u001b[39m, v_complicated_func(x_values))\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[27], line 23\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Compute the gradient of the function\u001b[39;00m\n\u001b[1;32m     22\u001b[0m x_values \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m2.0\u001b[39m, \u001b[38;5;241m4.0\u001b[39m, \u001b[38;5;241m5.0\u001b[39m])  \u001b[38;5;66;03m# Test inputs\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m grads \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mgrad(\u001b[38;5;28;01mlambda\u001b[39;00m x: jnp\u001b[38;5;241m.\u001b[39msum(\u001b[43mv_complicated_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))(x_values)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Print results\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFunction values:\u001b[39m\u001b[38;5;124m\"\u001b[39m, v_complicated_func(x_values))\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[27], line 7\u001b[0m, in \u001b[0;36mcomplicated_func\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcomplicated_func\u001b[39m(x):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Conditional operation: square if greater than 3, else multiply by 2\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3.\u001b[39m:\n\u001b[1;32m      8\u001b[0m         y \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/mldl_htwg/lib/python3.10/site-packages/jax/_src/core.py:1492\u001b[0m, in \u001b[0;36mconcretization_function_error.<locals>.error\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m   1491\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21merror\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[0;32m-> 1492\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m TracerBoolConversionError(arg)\n",
      "\u001b[0;31mTracerBoolConversionError\u001b[0m: Attempted boolean conversion of traced array with shape bool[]..\nThis BatchTracer with object id 5872364544 was created on line:\n  /var/folders/bk/0vv7sh9n43n3dm4fth1qw93r0000gq/T/ipykernel_84478/168064051.py:7 (complicated_func)\nSee https://jax.readthedocs.io/en/latest/errors.html#jax.errors.TracerBoolConversionError"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Function with conditions and loops\n",
    "def complicated_func(x):\n",
    "    # Conditional operation: square if greater than 3, else multiply by 2\n",
    "    if x > 3.:\n",
    "        y = x**2\n",
    "    else:\n",
    "        y = 2 * x\n",
    "    \n",
    "    # Use a loop to compute a cumulative sum\n",
    "    result = 0.0\n",
    "    for i in range(1, 6):  # Sum of i * y for i in range(1, 6)\n",
    "        result += i * y\n",
    "    return result.mean()\n",
    "\n",
    "# Vectorize the function for multiple inputs\n",
    "v_complicated_func = jax.vmap(complicated_func)\n",
    "\n",
    "# Compute the gradient of the function\n",
    "x_values = jnp.array([2.0, 4.0, 5.0])  # Test inputs\n",
    "grads = jax.grad(lambda x: jnp.sum(v_complicated_func(x)))(x_values)\n",
    "\n",
    "# Print results\n",
    "print(\"Function values:\", v_complicated_func(x_values))\n",
    "print(\"Gradients at test points:\", grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl_htwg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
